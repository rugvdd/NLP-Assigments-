{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0IH7yZXT2yO",
        "outputId": "13d927c7-7c42-406a-8650-99f3df7fa35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas scikit-learn nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"I love machine learning and artificial intelligence\",\n",
        "        \"This product is extremely bad and disappointing\",\n",
        "        \"Natural language processing is very interesting\",\n",
        "        \"I hate spam messages and fake calls\",\n",
        "        \"Deep learning models perform well on large data\",\n",
        "        \"The service quality was terrible and slow\",\n",
        "        \"AI is transforming healthcare and education\",\n",
        "        \"This app crashes frequently and is useless\",\n",
        "        \"Data science is an exciting career option\",\n",
        "        \"The customer support experience was awful\",\n",
        "        \"Machine learning improves decision making\",\n",
        "        \"The software has too many bugs\",\n",
        "        \"Artificial intelligence helps solve complex problems\",\n",
        "        \"I am unhappy with the poor performance\",\n",
        "        \"Technology advancements are beneficial to society\",\n",
        "        \"The interface design is confusing and frustrating\"\n",
        "    ],\n",
        "    \"label\": [\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "data.to_csv(\"dataset.csv\", index=False)\n",
        "print(\"✅ dataset.csv created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1T-e60oXJTD",
        "outputId": "bf56af21-1adb-4e2d-d2cf-49215a847a82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ dataset.csv created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"dataset.csv\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDzc3gJEXJid",
        "outputId": "c0f894c7-fb7f-4f1b-ca96-3366559bb180"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text     label\n",
            "0  I love machine learning and artificial intelli...  positive\n",
            "1    This product is extremely bad and disappointing  negative\n",
            "2    Natural language processing is very interesting  positive\n",
            "3                I hate spam messages and fake calls  negative\n",
            "4    Deep learning models perform well on large data  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# TEXT PREPROCESSING + TF-IDF\n",
        "# -------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "JK6SaYcmT7Yt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Download NLTK resources\n",
        "# -------------------------------\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-XIb2GbT_Xm",
        "outputId": "0f4f8cf5-d8f5-41fd-a242-7dd684e5219c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Load Dataset\n",
        "# -------------------------------\n",
        "# CSV must have columns: \"text\" and \"label\"\n",
        "data = pd.read_csv(\"dataset.csv\")"
      ],
      "metadata": {
        "id": "IlIfjXVMUEXc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Text Cleaning Function\n",
        "# -------------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # remove numbers & punctuation\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "DTwSOgWzV2or"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Apply Preprocessing\n",
        "# -------------------------------\n",
        "data[\"clean_text\"] = data[\"text\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "1UfIbyu9V6ru"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Label Encoding\n",
        "# -------------------------------\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"label_encoded\"] = label_encoder.fit_transform(data[\"label\"])"
      ],
      "metadata": {
        "id": "cyeMXmGIV9zj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# TF-IDF Vectorization\n",
        "# -------------------------------\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(data[\"clean_text\"])"
      ],
      "metadata": {
        "id": "-6rO9gpTWBKW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save cleaned dataset\n",
        "data.to_csv(\"cleaned_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "fdNITU7dWEPm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save TF-IDF matrix\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
        "tfidf_df.to_csv(\"tfidf_features.csv\", index=False)"
      ],
      "metadata": {
        "id": "i6VoxQvGWJoV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save label encoder\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)"
      ],
      "metadata": {
        "id": "c5tERHQfWMaY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save TF-IDF model\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf, f)"
      ],
      "metadata": {
        "id": "-SRRin1pWPQU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ Text processing and TF-IDF feature extraction completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5rKobRFWSIo",
        "outputId": "9788212a-2beb-44a6-92bb-a04ad03f6f61"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Text processing and TF-IDF feature extraction completed successfully!\n"
          ]
        }
      ]
    }
  ]
}