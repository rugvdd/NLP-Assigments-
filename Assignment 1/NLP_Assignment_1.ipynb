{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WuWreA07OIEC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (\n",
        "    WhitespaceTokenizer,\n",
        "    WordPunctTokenizer,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text containing:\n",
        "# 1. A contraction (don't)\n",
        "# 2. Punctuation (New York!)\n",
        "# 3. Social media tags (#NLP, @Rohan)\n",
        "# 4. A multi-word expression (Natural Language)\n",
        "text = \"I don't like #NLP? @Rohan said Natural Language is hard in New York!\""
      ],
      "metadata": {
        "id": "3fwJCUYBPAWb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Whitespace Tokenizer\n",
        "ws_tokens = WhitespaceTokenizer().tokenize(text)"
      ],
      "metadata": {
        "id": "eUDiIueRPDfG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Punctuation Tokenizer\n",
        "punct_tokens = WordPunctTokenizer().tokenize(text)"
      ],
      "metadata": {
        "id": "phJVwSKfPIBv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Treebank Tokenizer\n",
        "tree_tokens = TreebankWordTokenizer().tokenize(text)"
      ],
      "metadata": {
        "id": "WGEqnyYnPK5I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tweet Tokenizer\n",
        "tweet_tokens = TweetTokenizer().tokenize(text)"
      ],
      "metadata": {
        "id": "LbYUSKaEPLEJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. MWE(Multi-Word Expression) Tokenizer (Requires predefined expressions)\n",
        "mwe_tokenizer = MWETokenizer([('Natural', 'Language'), ('New', 'York')])\n",
        "# Note: MWE usually requires a pre-tokenized list\n",
        "mwe_tokens = mwe_tokenizer.tokenize(ws_tokens)"
      ],
      "metadata": {
        "id": "6yX-NKbAPToS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Results\n",
        "print(f\"Original: {text}\\n\" + \"-\"*30)\n",
        "print(f\"Whitespace: {ws_tokens}\")\n",
        "print(f\"Punctuation: {punct_tokens}\")\n",
        "print(f\"Treebank:   {tree_tokens}\")\n",
        "print(f\"Tweet:      {tweet_tokens}\")\n",
        "print(f\"MWE:        {mwe_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRCenxQIPYsJ",
        "outputId": "7bb20491-456d-4d5c-a6cd-9afac00ac763"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I don't like #NLP? @Rohan said Natural Language is hard in New York!\n",
            "------------------------------\n",
            "Whitespace: ['I', \"don't\", 'like', '#NLP?', '@Rohan', 'said', 'Natural', 'Language', 'is', 'hard', 'in', 'New', 'York!']\n",
            "Punctuation: ['I', 'don', \"'\", 't', 'like', '#', 'NLP', '?', '@', 'Rohan', 'said', 'Natural', 'Language', 'is', 'hard', 'in', 'New', 'York', '!']\n",
            "Treebank:   ['I', 'do', \"n't\", 'like', '#', 'NLP', '?', '@', 'Rohan', 'said', 'Natural', 'Language', 'is', 'hard', 'in', 'New', 'York', '!']\n",
            "Tweet:      ['I', \"don't\", 'like', '#NLP', '?', '@Rohan', 'said', 'Natural', 'Language', 'is', 'hard', 'in', 'New', 'York', '!']\n",
            "MWE:        ['I', \"don't\", 'like', '#NLP?', '@Rohan', 'said', 'Natural_Language', 'is', 'hard', 'in', 'New', 'York!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer"
      ],
      "metadata": {
        "id": "feAN93wtWaMD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize the stemmers\n",
        "porter = PorterStemmer()\n",
        "# Snowball requires you to specify the language\n",
        "snowball = SnowballStemmer(language='english')"
      ],
      "metadata": {
        "id": "c2m-CO1KWeIO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define a list of words to test\n",
        "# These include variations in tense, plurality, and adverbs\n",
        "words = [\n",
        "    \"generous\", \"generously\", \"generation\",\n",
        "    \"running\", \"ran\", \"runs\",\n",
        "    \"fairly\", \"fairness\",\n",
        "    \"ponies\", \"caresses\"\n",
        "]"
      ],
      "metadata": {
        "id": "w64dp5GDWhVW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Print the header for our comparison table\n",
        "print(f\"{'Original Word':<15} | {'Porter Stemmer':<15} | {'Snowball Stemmer':<15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Apply stemming and display results\n",
        "for word in words:\n",
        "    p_stem = porter.stem(word)\n",
        "    s_stem = snowball.stem(word)\n",
        "    print(f\"{word:<15} | {p_stem:<15} | {s_stem:<15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlVy19dbWhYA",
        "outputId": "0a1c2ebc-9638-4e6d-ca07-217639effd18"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Word   | Porter Stemmer  | Snowball Stemmer\n",
            "--------------------------------------------------\n",
            "generous        | gener           | generous       \n",
            "generously      | gener           | generous       \n",
            "generation      | gener           | generat        \n",
            "running         | run             | run            \n",
            "ran             | ran             | ran            \n",
            "runs            | run             | run            \n",
            "fairly          | fairli          | fair           \n",
            "fairness        | fair            | fair           \n",
            "ponies          | poni            | poni           \n",
            "caresses        | caress          | caress         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Demonstrating a phrase/sentence stemming\n",
        "sentence = \"The dogs are running quickly through the leaves\"\n",
        "sentence_tokens = sentence.split()\n",
        "stemmed_sentence = [snowball.stem(w) for w in sentence_tokens]\n",
        "\n",
        "print(\"\\nOriginal Sentence:\", sentence)\n",
        "print(\"Snowball Stemmed:\", \" \".join(stemmed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAOOqbUIWhat",
        "outputId": "804148a2-74ad-459a-f793-3ba65d711f9b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Sentence: The dogs are running quickly through the leaves\n",
            "Snowball Stemmed: the dog are run quick through the leav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "u-jUJxj8WhdT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the necessary datasets for WordNet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_gpnjslWhgF",
        "outputId": "af6136c1-5a70-4e9b-b8b2-7f9f575e22ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize the Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "zeIi6WxiWhiq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. List of words to test\n",
        "words = [\"feet\", \"cacti\", \"geese\", \"rocks\", \"running\", \"was\", \"better\"]"
      ],
      "metadata": {
        "id": "hA3XESZtWhl_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{'Original Word':<15} | {'Lemma (Default)':<15} | {'Lemma (with POS)':<15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Demonstrate the difference POS tags make\n",
        "# 'v' stands for verb, 'a' for adjective, 'n' for noun\n",
        "test_data = [\n",
        "    (\"running\", wordnet.VERB),\n",
        "    (\"was\", wordnet.VERB),\n",
        "    (\"better\", wordnet.ADJ),\n",
        "    (\"feet\", wordnet.NOUN),\n",
        "    (\"leaves\", wordnet.NOUN),\n",
        "    (\"leaves\", wordnet.VERB)\n",
        "]\n",
        "\n",
        "for word, tag in test_data:\n",
        "    # Without tag, WordNet assumes everything is a Noun\n",
        "    default_lemma = lemmatizer.lemmatize(word)\n",
        "    # With tag, it finds the true root\n",
        "    tagged_lemma = lemmatizer.lemmatize(word, pos=tag)\n",
        "\n",
        "    print(f\"{word:<15} | {default_lemma:<15} | {tagged_lemma:<15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNqZ-uRGXCAe",
        "outputId": "f9629de8-e6cf-4d62-9c52-4d8b7fa05db3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Word   | Lemma (Default) | Lemma (with POS)\n",
            "--------------------------------------------------\n",
            "running         | running         | run            \n",
            "was             | wa              | be             \n",
            "better          | better          | good           \n",
            "feet            | foot            | foot           \n",
            "leaves          | leaf            | leaf           \n",
            "leaves          | leaf            | leave          \n"
          ]
        }
      ]
    }
  ]
}